{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "check = True \n",
    "\n",
    "\n",
    "# load data \n",
    "\n",
    "# 개인 아이디어 : 주장을 넣었을 때, ChatGPT 에 근거를 넣기\n",
    "# 현재 모델은 어떤 단어가 들어갔는지만 고려. 깊게 발전시키지 않음. \n",
    "\n",
    "# 데이터를 보니, 의견을 알리는 것도 있음. 즉, 사실 주장을 증명하기엔 애매모호한 주제들이 많음. \n",
    "# 데이터셋도 필터링이 필요할 듯. 흐으음.. 한편으론 글의 타입이 변하기만 해도 애매한 부분이\n",
    "\n",
    "\n",
    "# load tokenize \n",
    "# make model & train \n",
    "# 학습한 모델 불러오기 -> 모델이 상당히 조잡한데... \n",
    "\n",
    "\n",
    "csv_location = f\"C:/Users/user/Desktop/test_ethic/20231110_total.csv\"\n",
    "\n",
    "\n",
    "def to_one_hot(sequences, dimension):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "df = pd.read_csv(csv_location, encoding='CP949')\n",
    "\n",
    "def embedding(data, max_words, word_index = None) : \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    # Tokenizer 내부의 단어-인덱스 매핑을 구축함. \n",
    "    tokenizer.fit_on_texts(data)\n",
    "\n",
    "    # 단어 - 인덱스 매핑 저장하기 \n",
    "    if word_index is None : \n",
    "        new_word_index = tokenizer.word_index\n",
    "    else : \n",
    "        tokenizer.word_index = word_index\n",
    "    \n",
    "\n",
    "    # texts 에 담겨있는 텍스트 데이터를 정수 시퀀스로 변환\n",
    "    train_data = tokenizer.texts_to_sequences(data) \n",
    "    if word_index is None : \n",
    "        return new_word_index, train_data \n",
    "    else : \n",
    "        return word_index, train_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_data(csv_location) : \n",
    "check = True \n",
    "if check == True : \n",
    "    df = pd.read_csv(csv_location, encoding='CP949')\n",
    "    #df['label'] = df['label'].replace({'판단 유보' : -1, '사실' : 2, '대체로 사실': 2, '대체로 사실 아님' : 0, '전혀 사실 아님' : 0, '논쟁 중' : -1, '절반의 사실' : 1, '판단 유보' : 1})\n",
    "    df['label'] = df['label'].replace({'판단 유보' : 1, '사실' : 2, '대체로 사실': 2, '대체로 사실 아님' : 0, '전혀 사실 아님' : 0, '논쟁 중' : 1, '절반의 사실' : 1, '판단 유보' : 1})\n",
    "\n",
    "\n",
    "    filtered_df = df[df['label'] != -1]\n",
    "\n",
    "    train_texts = filtered_df[\"제목\"].tolist()\n",
    "\n",
    "    train_label = filtered_df['label'].tolist()\n",
    "    \n",
    "    train_label = np.asarray(train_label).astype('float32')\n",
    "    test_ratio = math.floor(len(train_texts) * 0.2)\n",
    "\n",
    "    max_words = 1000\n",
    "    maxlen = 1000\n",
    "\n",
    "    word_index, train_data = embedding(data = train_texts, max_words=max_words)    \n",
    "    ont_hot_train_data = to_one_hot(train_data, dimension=max_words)\n",
    "    \n",
    "    x_train = ont_hot_train_data[test_ratio:]\n",
    "    y_train = train_label[test_ratio:]\n",
    "    y_train_one_hot = to_categorical(y_train, num_classes=3)\n",
    "\n",
    "\n",
    "    x_val = ont_hot_train_data[:test_ratio]\n",
    "    y_val = train_label[:test_ratio]\n",
    "    y_val_one_hot = to_categorical(y_val, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>출처</th>\n",
       "      <th>label</th>\n",
       "      <th>링크</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>길거리에서 자신의 가슴을 만져달라는 퍼포먼스는 공연음란죄로 처벌된다</td>\n",
       "      <td>출처: 인터넷 커뮤니티</td>\n",
       "      <td>1</td>\n",
       "      <td>https://bbs.ruliweb.com/community/board/300143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리나라 공공의료 비중은 OECD 국가 중 꼴찌다</td>\n",
       "      <td>출처: 국립중앙의료원 이전신축 축소, 국감서도 관심…\"병상 확보돼야\"</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.medipana.com/article/view.php?news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>빈대 방역 위해 뿌리는 규조토 분말, 인체에 해롭지 않다</td>\n",
       "      <td>출처: 인터넷 커뮤니티 게시물(루리웹, 2023년 11월 3일)</td>\n",
       "      <td>0</td>\n",
       "      <td>https://bbs.ruliweb.com/community/board/300148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>우리나라는 허위정보 발언에 대한 국회의원 면책특권 범위가 해외보다 넓다</td>\n",
       "      <td>출처: 조선일보 [사설] 가짜 뉴스로 사익까지 챙길 수 있는 한국 의원의 특권(20...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.chosun.com/opinion/editorial/2023/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>수도권 제4매립지가 김포 땅이어서 쓰레기 문제를 해결할 수 있다</td>\n",
       "      <td>출처: 매일경제 김병수 김포시장 인터뷰</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.mk.co.kr/news/society/10849275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4644</th>\n",
       "      <td>\"더불어민주당도 국모닝 했다\"</td>\n",
       "      <td>출처: '썰전' 안철수 국민의당은 '문모닝'당? 정책발표는 기사가 안나와</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.mydaily.co.kr/new_yk/html/read.php?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4645</th>\n",
       "      <td>“77%로 미군 주둔비 부담 커…일본은 50%”</td>\n",
       "      <td>출처: 이재명 시장, \"주한 미군 주둔비 적정선에서 조정 필요\"</td>\n",
       "      <td>0</td>\n",
       "      <td>http://news.joins.com/article/21073540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>현재 지지율 1위 문재인 민주당 경선 후보 아들과 관련한 취업특혜 논란이 있다.</td>\n",
       "      <td>출처:</td>\n",
       "      <td>1</td>\n",
       "      <td>url not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647</th>\n",
       "      <td>‘검수완박’ 추진은 헌법에 위반된다</td>\n",
       "      <td>출처: 관련보도(한국일보)</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.hankookilbo.com/News/Read/A2022041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>대한제국공사관에 태극기가 뒤집혀 걸려있다?</td>\n",
       "      <td>출처: 심재철 의원 페이스북(2018년 5월23일)</td>\n",
       "      <td>0</td>\n",
       "      <td>url not available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4649 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                제목   \n",
       "0            길거리에서 자신의 가슴을 만져달라는 퍼포먼스는 공연음란죄로 처벌된다  \\\n",
       "1                      우리나라 공공의료 비중은 OECD 국가 중 꼴찌다   \n",
       "2                  빈대 방역 위해 뿌리는 규조토 분말, 인체에 해롭지 않다   \n",
       "3          우리나라는 허위정보 발언에 대한 국회의원 면책특권 범위가 해외보다 넓다   \n",
       "4              수도권 제4매립지가 김포 땅이어서 쓰레기 문제를 해결할 수 있다   \n",
       "...                                            ...   \n",
       "4644                              \"더불어민주당도 국모닝 했다\"   \n",
       "4645                    “77%로 미군 주둔비 부담 커…일본은 50%”   \n",
       "4646  현재 지지율 1위 문재인 민주당 경선 후보 아들과 관련한 취업특혜 논란이 있다.   \n",
       "4647                           ‘검수완박’ 추진은 헌법에 위반된다   \n",
       "4648                       대한제국공사관에 태극기가 뒤집혀 걸려있다?   \n",
       "\n",
       "                                                     출처  label   \n",
       "0                                          출처: 인터넷 커뮤니티      1  \\\n",
       "1                출처: 국립중앙의료원 이전신축 축소, 국감서도 관심…\"병상 확보돼야\"      2   \n",
       "2                   출처: 인터넷 커뮤니티 게시물(루리웹, 2023년 11월 3일)      0   \n",
       "3     출처: 조선일보 [사설] 가짜 뉴스로 사익까지 챙길 수 있는 한국 의원의 특권(20...      2   \n",
       "4                                 출처: 매일경제 김병수 김포시장 인터뷰      0   \n",
       "...                                                 ...    ...   \n",
       "4644           출처: '썰전' 안철수 국민의당은 '문모닝'당? 정책발표는 기사가 안나와      1   \n",
       "4645                출처: 이재명 시장, \"주한 미군 주둔비 적정선에서 조정 필요\"      0   \n",
       "4646                                                출처:      1   \n",
       "4647                                     출처: 관련보도(한국일보)      1   \n",
       "4648                       출처: 심재철 의원 페이스북(2018년 5월23일)      0   \n",
       "\n",
       "                                                     링크  \n",
       "0     https://bbs.ruliweb.com/community/board/300143...  \n",
       "1     https://www.medipana.com/article/view.php?news...  \n",
       "2     https://bbs.ruliweb.com/community/board/300148...  \n",
       "3     https://www.chosun.com/opinion/editorial/2023/...  \n",
       "4            https://www.mk.co.kr/news/society/10849275  \n",
       "...                                                 ...  \n",
       "4644  http://www.mydaily.co.kr/new_yk/html/read.php?...  \n",
       "4645             http://news.joins.com/article/21073540  \n",
       "4646                                  url not available  \n",
       "4647  https://www.hankookilbo.com/News/Read/A2022041...  \n",
       "4648                                  url not available  \n",
       "\n",
       "[4649 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1000, 50)          50000     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 50000)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                3200064   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,250,259\n",
      "Trainable params: 3,250,259\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 2.6946 - accuracy: 0.5245 - val_loss: 1.0763 - val_accuracy: 0.4865\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.9732 - accuracy: 0.5884 - val_loss: 1.0543 - val_accuracy: 0.4865\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.9739 - accuracy: 0.5884 - val_loss: 1.0632 - val_accuracy: 0.4865\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.9449 - accuracy: 0.5884 - val_loss: 1.0916 - val_accuracy: 0.4865\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.8928 - accuracy: 0.5884 - val_loss: 1.0956 - val_accuracy: 0.4865\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.8609 - accuracy: 0.5882 - val_loss: 1.2051 - val_accuracy: 0.4865\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.8252 - accuracy: 0.6030 - val_loss: 1.1924 - val_accuracy: 0.4349\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.8079 - accuracy: 0.6223 - val_loss: 1.2455 - val_accuracy: 0.4672\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.7856 - accuracy: 0.6468 - val_loss: 1.3220 - val_accuracy: 0.4747\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.7618 - accuracy: 0.6516 - val_loss: 1.3975 - val_accuracy: 0.4446\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.7478 - accuracy: 0.6513 - val_loss: 1.3242 - val_accuracy: 0.3789\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.8479 - accuracy: 0.5801 - val_loss: 1.4640 - val_accuracy: 0.3681\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.8498 - accuracy: 0.5935 - val_loss: 1.2763 - val_accuracy: 0.4112\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.7845 - accuracy: 0.6180 - val_loss: 1.9293 - val_accuracy: 0.4521\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.8384 - accuracy: 0.5957 - val_loss: 1.3266 - val_accuracy: 0.4015\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.7847 - accuracy: 0.6172 - val_loss: 1.3246 - val_accuracy: 0.3800\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.7719 - accuracy: 0.6223 - val_loss: 1.4060 - val_accuracy: 0.4370\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.7593 - accuracy: 0.6382 - val_loss: 1.4657 - val_accuracy: 0.4338\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.7581 - accuracy: 0.6336 - val_loss: 1.8200 - val_accuracy: 0.3197\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.7696 - accuracy: 0.6247 - val_loss: 1.6592 - val_accuracy: 0.4586\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.7584 - accuracy: 0.6349 - val_loss: 1.7605 - val_accuracy: 0.4575\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.7538 - accuracy: 0.6344 - val_loss: 1.6844 - val_accuracy: 0.4392\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.7492 - accuracy: 0.6325 - val_loss: 1.6246 - val_accuracy: 0.4392\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.7399 - accuracy: 0.6473 - val_loss: 1.6233 - val_accuracy: 0.4241\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.7386 - accuracy: 0.6435 - val_loss: 1.6630 - val_accuracy: 0.4403\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.7391 - accuracy: 0.6438 - val_loss: 1.8377 - val_accuracy: 0.4413\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.7322 - accuracy: 0.6511 - val_loss: 1.8445 - val_accuracy: 0.4467\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.7129 - accuracy: 0.6543 - val_loss: 1.7569 - val_accuracy: 0.4144\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.7157 - accuracy: 0.6616 - val_loss: 1.8088 - val_accuracy: 0.4327\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.7015 - accuracy: 0.6608 - val_loss: 1.8416 - val_accuracy: 0.4392\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.6864 - accuracy: 0.6680 - val_loss: 1.8377 - val_accuracy: 0.4327\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 0.6810 - accuracy: 0.6761 - val_loss: 2.0277 - val_accuracy: 0.4360\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.6684 - accuracy: 0.6876 - val_loss: 1.9131 - val_accuracy: 0.4177\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.6581 - accuracy: 0.7022 - val_loss: 1.9721 - val_accuracy: 0.4241\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.6382 - accuracy: 0.7126 - val_loss: 2.0433 - val_accuracy: 0.4112\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.6140 - accuracy: 0.7274 - val_loss: 2.4000 - val_accuracy: 0.4273\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.6100 - accuracy: 0.7339 - val_loss: 2.3424 - val_accuracy: 0.4241\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.5856 - accuracy: 0.7473 - val_loss: 2.6585 - val_accuracy: 0.4370\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.5807 - accuracy: 0.7449 - val_loss: 2.3609 - val_accuracy: 0.4306\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.5393 - accuracy: 0.7667 - val_loss: 2.6674 - val_accuracy: 0.4273\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.5248 - accuracy: 0.7763 - val_loss: 2.9601 - val_accuracy: 0.4037\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.5906 - accuracy: 0.7503 - val_loss: 2.6946 - val_accuracy: 0.4166\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.5397 - accuracy: 0.7581 - val_loss: 2.8685 - val_accuracy: 0.4155\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.5130 - accuracy: 0.7745 - val_loss: 2.7702 - val_accuracy: 0.4349\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.4809 - accuracy: 0.7863 - val_loss: 2.9953 - val_accuracy: 0.4306\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.4715 - accuracy: 0.7930 - val_loss: 3.1340 - val_accuracy: 0.4198\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.4527 - accuracy: 0.8046 - val_loss: 3.1080 - val_accuracy: 0.4241\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.4340 - accuracy: 0.8121 - val_loss: 3.4593 - val_accuracy: 0.4284\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.4297 - accuracy: 0.8121 - val_loss: 3.5773 - val_accuracy: 0.4080\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.4165 - accuracy: 0.8207 - val_loss: 3.6777 - val_accuracy: 0.4133\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if check == True : \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=50, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    #model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    custom_optimizer = Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(x_train, y_train_one_hot, epochs=50, batch_size=128,  validation_data=(x_val, y_val_one_hot))\n",
    "    \n",
    "    model.save('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "# 모델 로드\n",
    "model = load_model('test.h5')\n",
    "\n",
    "test_df = pd.read_csv(csv_location, encoding='CP949')\n",
    "test_data = random.sample(test_df['제목'].tolist(), 100)\n",
    "\n",
    "word_index, embed_data = embedding(data = test_data, max_words=max_words, word_index=word_index)    \n",
    "ont_hot_train_data = to_one_hot(embed_data, dimension=max_words)\n",
    "\n",
    "\n",
    "# 새로운 테스트 데이터셋에 대한 예측\n",
    "predictions = model.predict(ont_hot_train_data)  # new_x_test는 새로운 테스트 데이터셋\n",
    "max_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# x_val 부분은 사전 전처리 코드를 추가해둬야겠네. \n",
    "# embedding 전의 데이터와 구분할 필요가 있겠다. \n",
    "\n",
    "#temporary_category = ['조선일보' for index in max_indices]\n",
    "categories = ['JTBC', 'KBS', 'MBC', 'SBS']\n",
    "temporary_category = [random.choice(categories) for _ in max_indices]\n",
    "\n",
    "# 예측 결과와 테스트 데이터를 DataFrame으로 만들기\n",
    "\n",
    "label_values = ['거짓', '논란', '사실']  # 실제 레이블에 맞게 수정\n",
    "mapped_labels = [label_values[prediction] for prediction in max_indices]\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame({'test_data': test_data, 'Category' : temporary_category ,'prediction': mapped_labels})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JTBC',\n",
       " 'JTBC',\n",
       " 'SBS',\n",
       " 'MBC',\n",
       " 'JTBC',\n",
       " 'MBC',\n",
       " 'JTBC',\n",
       " 'KBS',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'MBC',\n",
       " 'SBS',\n",
       " 'JTBC',\n",
       " 'MBC',\n",
       " 'MBC',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'JTBC',\n",
       " 'JTBC',\n",
       " 'MBC',\n",
       " 'MBC',\n",
       " 'JTBC',\n",
       " 'JTBC',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'KBS',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'KBS',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'SBS',\n",
       " 'MBC',\n",
       " 'JTBC',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'MBC',\n",
       " 'MBC',\n",
       " 'MBC',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'JTBC',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'MBC',\n",
       " 'MBC',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'KBS',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'JTBC',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'MBC',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'JTBC',\n",
       " 'KBS',\n",
       " 'JTBC',\n",
       " 'JTBC',\n",
       " 'MBC',\n",
       " 'MBC',\n",
       " 'MBC',\n",
       " 'KBS',\n",
       " 'KBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'SBS',\n",
       " 'KBS',\n",
       " 'MBC']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temporary_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 함수를 사용하여 확률 분포 얻기\n",
    "probabilities = model.predict(ont_hot_train_data, verbose=0)\n",
    "\n",
    "# 각 예측에서 가장 높은 확률을 가진 클래스의 인덱스 가져오기\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# 예측된 레이블을 실제 레이블 값으로 변환 (예: 0부터 시작하는 클래스 레이블)\n",
    "# 여기서는 가정적으로 label_values를 정의하여 사용합니다. 실제로는 데이터셋에 따라 다르게 설정될 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f\"total_result.csv\", encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Num_Data  Num_Fact  Num_Conv  Num_Fake\n",
      "0      SBS        27         7         6        14\n",
      "1      MBC        25         7         7        11\n",
      "2      KBS        26         8         3        15\n",
      "3     JTBC        22        10         4         8\n"
     ]
    }
   ],
   "source": [
    "category = list(set(result_df['Category'].tolist()))\n",
    "\n",
    "info = []\n",
    "for c in category : \n",
    "    filtered_df = result_df[result_df['Category'] == c]\n",
    "    num_data = len(filtered_df)\n",
    "    num_fact = len(filtered_df[filtered_df['prediction'] == '사실'])\n",
    "    num_conv = len(filtered_df[filtered_df['prediction'] == '논란'])\n",
    "    num_fake = len(filtered_df[filtered_df['prediction'] == '거짓'])\n",
    "\n",
    "    info.append([c, num_data, num_fact, num_conv, num_fake])\n",
    "\n",
    "\n",
    "columns = ['Category', 'Num_Data', 'Num_Fact', 'Num_Conv', 'Num_Fake']\n",
    "df_info = pd.DataFrame(info, columns=columns)\n",
    "\n",
    "print(df_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.to_csv(f\"result_per_category.csv\", encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
